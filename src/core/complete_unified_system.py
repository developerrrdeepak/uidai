"""
COMPLETE UIDAI UNIFIED SYSTEM
============================

This file connects all existing code into one complete solution:
- Model 1: Anomaly Detection
- Model 2: Risk Scoring  
- Model 3: Risk Classification
- Data Processing
- Policy Recommendations
- Dashboard Integration
"""

import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# MODEL 3 RISK CLASSIFIER (From existing code)
# ============================================================================

class Model3RiskClassifier:
    """Model 3: Risk Classification with ML validation"""
    
    def __init__(self):
        self.risk_types = {
            'Child Exclusion Risk': 'Anganwadi-based enrolment drives',
            'Gender Gap Risk': 'Women-only enrolment camps', 
            'Update Failure Risk': 'Mobile update camps',
            'Administrative Disruption Risk': 'Staff reallocation',
            'Migration / Crisis Shock Risk': 'Emergency portable services'
        }
    
    def classify_batch(self, df):
        """Classify multiple districts at once"""
        results = []
        
        for _, row in df.iterrows():
            classification = self._classify_single_district(row)
            results.append(classification)
        
        return pd.DataFrame(results)
    
    def _classify_single_district(self, row):
        """Classify a single district"""
        
        # Determine risk type based on anomalies
        risk_type, reason, action = self._determine_risk_type(row)
        
        # Calculate severity score
        severity_score = self._calculate_severity_score(row)
        
        # Determine severity level
        if severity_score >= 0.7:
            severity = 'High'
        elif severity_score >= 0.4:
            severity = 'Medium'
        else:
            severity = 'Low'
        
        return {
            'District': row['district'],
            'Risk Type': risk_type,
            'Severity': severity,
            'Severity Score': severity_score,
            'Reason': reason,
            'Suggested Action': action
        }
    
    def _determine_risk_type(self, row):
        """Determine risk type based on patterns"""
        
        # Migration/Crisis (highest priority)
        if row.get('sudden_spike', False) or row.get('sudden_large_drop', False):
            if row.get('sudden_spike', False):
                return ('Migration / Crisis Shock Risk', 
                       f"Sudden spike: {row['enrolment_pct_change']:.0f}% increase", 
                       'Emergency portable services')
            else:
                return ('Migration / Crisis Shock Risk', 
                       f"Mass exodus: {row['enrolment_pct_change']:.0f}% drop", 
                       'Emergency portable services')
        
        # Child Exclusion
        if row.get('child_anomaly', False):
            return ('Child Exclusion Risk', 
                   f"Child enrolments down {abs(row['child_pct_change']):.0f}%", 
                   'Anganwadi-based enrolment drives')
        
        # Gender Gap
        if row.get('female_anomaly', False):
            return ('Gender Gap Risk', 
                   f"Female enrolments down {abs(row['female_pct_change']):.0f}%", 
                   'Women-only enrolment camps')
        
        # Update Failure
        if row.get('update_anomaly', False):
            return ('Update Failure Risk', 
                   f"Updates down {abs(row['update_pct_change']):.0f}%", 
                   'Mobile update camps')
        
        # Administrative Issues
        if row.get('enrolment_anomaly', False):
            return ('Administrative Disruption Risk', 
                   f"Enrolments down {abs(row['enrolment_pct_change']):.0f}%", 
                   'Staff reallocation')
        
        return ('Low Risk', 'Normal operations', 'Continue monitoring')
    
    def _calculate_severity_score(self, row):
        """Calculate severity score based on multiple factors"""
        
        # Base score from anomaly severity
        base_score = row.get('anomaly_severity', 0.0)
        
        # Structural risk component
        structural_score = row.get('structural_risk_score', 0.0)
        
        # Combined score
        severity_score = base_score * 0.6 + structural_score * 0.4
        
        return np.clip(severity_score, 0, 1)
    
    def get_summary_statistics(self, results):
        """Get summary statistics from classification results"""
        
        total_districts = len(results)
        
        # Risk distribution
        risk_distribution = results['Risk Type'].value_counts().to_dict()
        
        # Severity distribution
        severity_distribution = results['Severity'].value_counts().to_dict()
        
        # Average severity score
        avg_severity_score = results['Severity Score'].mean()
        
        return {
            'total_districts': total_districts,
            'avg_severity_score': avg_severity_score,
            'risk_distribution': risk_distribution,
            'severity_distribution': severity_distribution,
            'high_risk_districts': severity_distribution.get('High', 0),
            'medium_risk_districts': severity_distribution.get('Medium', 0),
            'low_risk_districts': severity_distribution.get('Low', 0)
        }


class Model3MLValidator:
    """ML Validator for Model 3 classifications"""
    
    def __init__(self, max_depth=3):
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.tree import export_text
        
        self.model = DecisionTreeClassifier(max_depth=max_depth, random_state=42)
        self.feature_names = None
        self.is_trained = False
    
    def train(self, df, target_labels):
        """Train ML validator on rule-based results"""
        
        # Prepare features
        feature_cols = [
            'enrolment_pct_change', 'update_pct_change', 
            'child_pct_change', 'female_pct_change',
            'anomaly_severity', 'structural_risk_score'
        ]
        
        X = df[feature_cols].fillna(0)
        y = target_labels
        
        self.feature_names = feature_cols
        self.model.fit(X, y)
        self.is_trained = True
    
    def predict(self, df):
        """Generate ML predictions"""
        if not self.is_trained:
            raise ValueError("Model must be trained first")
        
        X = df[self.feature_names].fillna(0)
        return self.model.predict(X)
    
    def validate_against_rules(self, rule_predictions, ml_predictions):
        """Compare rule-based vs ML predictions"""
        
        agreements = sum(rule_predictions == ml_predictions)
        total_cases = len(rule_predictions)
        disagreements = total_cases - agreements
        agreement_rate = agreements / total_cases
        
        validation_status = "GOOD" if agreement_rate >= 0.8 else "NEEDS_REVIEW"
        
        return {
            'agreement_rate': agreement_rate,
            'agreements': agreements,
            'total_cases': total_cases,
            'disagreements': disagreements,
            'validation_status': validation_status
        }
    
    def get_feature_importance(self):
        """Get feature importance from trained model"""
        if not self.is_trained:
            return pd.DataFrame()
        
        importance_df = pd.DataFrame({
            'Feature': self.feature_names,
            'Importance': self.model.feature_importances_
        }).sort_values('Importance', ascending=False)
        
        return importance_df
    
    def export_tree_rules(self):
        """Export decision tree rules as text"""
        if not self.is_trained:
            return "Model not trained"
        
        from sklearn.tree import export_text
        return export_text(self.model, feature_names=self.feature_names)


# ============================================================================
# COMPLETE INTEGRATED PIPELINE
# ============================================================================

class Model1AnomalyDetector:
    """Model 1: Anomaly Detection"""
    
    def detect_anomalies(self, df):
        """Detect anomalies in UIDAI data"""
        results = df.copy()
        
        # Enrolment anomalies
        results['enrolment_anomaly'] = (
            (results['enrolment_pct_change'] > 30) | 
            (results['enrolment_pct_change'] < -30)
        )
        
        # Update anomalies  
        results['update_anomaly'] = (
            (results['update_pct_change'] < -25)
        )
        
        # Child anomalies
        results['child_anomaly'] = (
            (results['child_pct_change'] < -20)
        )
        
        # Female anomalies
        results['female_anomaly'] = (
            (results['female_pct_change'] < -15)
        )
        
        # Sudden spikes/drops
        results['sudden_spike'] = results['enrolment_pct_change'] > 40
        results['sudden_large_drop'] = results['enrolment_pct_change'] < -40
        
        return results


class Model2RiskScoring:
    """Model 2: Risk Scoring"""
    
    def calculate_risk_score(self, df):
        """Calculate risk scores for districts"""
        results = df.copy()
        
        # Anomaly severity (0-1)
        anomaly_count = (
            df['enrolment_anomaly'].astype(int) +
            df['update_anomaly'].astype(int) +
            df['child_anomaly'].astype(int) +
            df['female_anomaly'].astype(int)
        )
        results['anomaly_severity'] = np.clip(anomaly_count / 4.0, 0, 1)
        
        # Structural risk (based on percentage changes)
        enrol_risk = np.clip(abs(df['enrolment_pct_change']) / 50.0, 0, 1)
        update_risk = np.clip(abs(df['update_pct_change']) / 40.0, 0, 1)
        child_risk = np.clip(abs(df['child_pct_change']) / 30.0, 0, 1)
        female_risk = np.clip(abs(df['female_pct_change']) / 25.0, 0, 1)
        
        results['structural_risk_score'] = (
            enrol_risk * 0.4 + update_risk * 0.3 + 
            child_risk * 0.2 + female_risk * 0.1
        )
        
        # Final risk score
        results['risk_score'] = np.clip(
            results['anomaly_severity'] * 0.6 + 
            results['structural_risk_score'] * 0.4, 
            0, 1
        )
        
        return results


class UIDaiCompleteSystem:
    """Complete UIDAI System - All Models Integrated"""
    
    def __init__(self):
        self.model1 = Model1AnomalyDetector()
        self.model2 = Model2RiskScoring()
        self.model3 = Model3RiskClassifier()
    
    def process_complete_pipeline(self, raw_data):
        """Complete pipeline: Raw Data â†’ Final Results"""
        
        print("Starting Complete UIDAI Pipeline...")
        
        # Step 1: Anomaly Detection
        print("Step 1: Anomaly Detection...")
        anomaly_data = self.model1.detect_anomalies(raw_data)
        
        # Step 2: Risk Scoring
        print("Step 2: Risk Scoring...")
        risk_data = self.model2.calculate_risk_score(anomaly_data)
        
        # Step 3: Risk Classification
        print("Step 3: Risk Classification...")
        classification_results = self.model3.classify_batch(risk_data)
        
        # Step 4: Create Final Alert Table
        print("Step 4: Creating Final Alert Table...")
        final_table = self._create_unified_table(risk_data, classification_results)
        
        print("Pipeline Complete!")
        return final_table
    
    def _create_unified_table(self, risk_data, classification_results):
        """Create unified hero output table"""
        
        # Merge risk data with classification results
        final_table = pd.DataFrame({
            'District': classification_results['District'],
            'Risk Score': risk_data['risk_score'].round(2),
            'Risk Level': classification_results['Severity'],
            'Alert': classification_results['Severity'].map({
                'High': 'Yes', 'Medium': 'Monitor', 'Low': 'No'
            }),
            'Risk Type': classification_results['Risk Type'],
            'Reason': classification_results['Reason'],
            'Action': classification_results['Suggested Action']
        })
        
        # Sort by risk score (highest first)
        final_table = final_table.sort_values('Risk Score', ascending=False)
        final_table = final_table.reset_index(drop=True)
        
        return final_table


# ============================================================================
# DATA GENERATION AND UTILITIES
# ============================================================================

def create_sample_data(n_districts=50):
    """Create sample UIDAI data for testing with districts from multiple states"""

    # Comprehensive list of districts from major Indian states
    all_districts = [
        # Odisha (Eastern India)
        'Kalahandi', 'Koraput', 'Rayagada', 'Gajapati', 'Kandhamal', 'Bolangir',
        'Nuapada', 'Bargarh', 'Jharsuguda', 'Sambalpur', 'Dhenkanal', 'Angul',
        'Deogarh', 'Sundargarh', 'Keonjhar', 'Mayurbhanj', 'Balasore', 'Bhadrak',

        # Maharashtra (Western India)
        'Mumbai', 'Pune', 'Nagpur', 'Nashik', 'Aurangabad', 'Thane', 'Solapur',
        'Kolhapur', 'Sangli', 'Satara', 'Ahmednagar', 'Dhule', 'Jalgaon',

        # Karnataka (Southern India)
        'Bangalore', 'Mysore', 'Mangalore', 'Hubli', 'Belgaum', 'Gulbarga',
        'Bijapur', 'Raichur', 'Bellary', 'Davangere', 'Shimoga', 'Tumkur',

        # Tamil Nadu (Southern India)
        'Chennai', 'Coimbatore', 'Madurai', 'Tiruchirappalli', 'Salem', 'Tirunelveli',
        'Tiruppur', 'Vellore', 'Thoothukkudi', 'Thanjavur', 'Kanyakumari', 'Erode',

        # Uttar Pradesh (Northern India)
        'Lucknow', 'Kanpur', 'Varanasi', 'Agra', 'Meerut', 'Allahabad', 'Ghaziabad',
        'Moradabad', 'Aligarh', 'Saharanpur', 'Bareilly', 'Gorakhpur', 'Faizabad',

        # West Bengal (Eastern India)
        'Kolkata', 'Howrah', 'Durgapur', 'Asansol', 'Siliguri', 'Kharagpur',
        'Bardhaman', 'Medinipur', 'Jalpaiguri', 'Darjeeling', 'Murshidabad', 'Nadia',

        # Gujarat (Western India)
        'Ahmedabad', 'Surat', 'Vadodara', 'Rajkot', 'Bhavnagar', 'Jamnagar',
        'Junagadh', 'Gandhinagar', 'Anand', 'Bharuch', 'Kheda', 'Mehsana',

        # Rajasthan (Northern India)
        'Jaipur', 'Jodhpur', 'Udaipur', 'Kota', 'Ajmer', 'Bikaner', 'Alwar',
        'Bhilwara', 'Chittorgarh', 'Sikar', 'Tonk', 'Bundi', 'Barmer',

        # Madhya Pradesh (Central India)
        'Bhopal', 'Indore', 'Jabalpur', 'Gwalior', 'Ujjain', 'Sagar', 'Rewa',
        'Satna', 'Ratlam', 'Dewas', 'Shajapur', 'Vidisha', 'Chhindwara',

        # Bihar (Eastern India)
        'Patna', 'Gaya', 'Bhagalpur', 'Muzaffarpur', 'Darbhanga', 'Purnia',
        'Bihar Sharif', 'Arrah', 'Begusarai', 'Katihar', 'Munger', 'Chhapra',

        # Andhra Pradesh (Southern India)
        'Hyderabad', 'Visakhapatnam', 'Vijayawada', 'Guntur', 'Nellore', 'Kurnool',
        'Rajahmundry', 'Tirupati', 'Kadapa', 'Anantapur', 'Chittoor', 'Eluru',

        # Kerala (Southern India)
        'Thiruvananthapuram', 'Kochi', 'Kozhikode', 'Thrissur', 'Kollam', 'Palakkad',
        'Kannur', 'Alappuzha', 'Kottayam', 'Idukki', 'Ernakulam', 'Malappuram',

        # Punjab (Northern India)
        'Ludhiana', 'Amritsar', 'Jalandhar', 'Patiala', 'Bathinda', 'Hoshiarpur',
        'Mohali', 'Firozpur', 'Moga', 'Faridkot', 'Sangrur', 'Barnala',

        # Haryana (Northern India)
        'Gurugram', 'Faridabad', 'Panipat', 'Ambala', 'Hisar', 'Karnal', 'Sonipat',
        'Rohtak', 'Yamunanagar', 'Panchkula', 'Sirsa', 'Jind', 'Fatehabad',

        # Delhi (Northern India)
        'New Delhi', 'North Delhi', 'South Delhi', 'East Delhi', 'West Delhi', 'Central Delhi',

        # Uttarakhand (Northern India)
        'Dehradun', 'Haridwar', 'Roorkee', 'Haldwani', 'Rudrapur', 'Kashipur',
        'Rishikesh', 'Kotdwar', 'Ramanagar', 'Pithoragarh', 'Almora', 'Nainital',

        # Himachal Pradesh (Northern India)
        'Shimla', 'Mandi', 'Solan', 'Dharamshala', 'Kullu', 'Hamirpur', 'Kangra',
        'Una', 'Bilaspur', 'Chamba', 'Lahaul and Spiti', 'Kinnaur', 'Sirmaur',

        # Jammu and Kashmir (Northern India)
        'Srinagar', 'Jammu', 'Anantnag', 'Baramulla', 'Pulwama', 'Kupwara',
        'Badgam', 'Doda', 'Ramban', 'Kathua', 'Poonch', 'Rajouri',

        # Goa (Western India)
        'Panaji', 'Margao', 'Vasco da Gama', 'Ponda', 'Mapusa', 'Bicholim',

        # Union Territories
        'Chandigarh', 'Puducherry', 'Daman', 'Diu', 'Lakshadweep', 'Andaman and Nicobar'
    ]

    districts = all_districts[:n_districts]
    
    np.random.seed(42)
    
    data = {
        'district': districts,
        'enrolment_pct_change': np.random.normal(0, 20, n_districts),
        'update_pct_change': np.random.normal(-5, 15, n_districts),
        'child_pct_change': np.random.normal(2, 18, n_districts),
        'female_pct_change': np.random.normal(1, 12, n_districts)
    }
    
    # Add some crisis scenarios for testing
    if n_districts >= 3:
        data['enrolment_pct_change'][0] = -38  # Kalahandi crisis
        data['update_pct_change'][0] = -35
        data['child_pct_change'][1] = -25      # Koraput child exclusion
        data['female_pct_change'][2] = -20     # Rayagada gender gap
    
    return pd.DataFrame(data)


def load_real_uidai_data():
    """Load real UIDAI data from CSV files"""
    try:
        # Try to load real data files
        enrolment_files = [
            'data/raw/api_data_aadhar_enrolment_0_500000.csv',
            'data/raw/api_data_aadhar_enrolment_500000_1000000.csv', 
            'data/raw/api_data_aadhar_enrolment_1000000_1006029.csv'
        ]
        
        dataframes = []
        for file in enrolment_files:
            try:
                df = pd.read_csv(file)
                dataframes.append(df)
            except FileNotFoundError:
                continue
        
        if dataframes:
            combined_df = pd.concat(dataframes, ignore_index=True)
            
            # Process real data into required format
            processed_data = process_real_data(combined_df)
            return processed_data
        else:
            print("Real data files not found, using sample data")
            return create_sample_data(15)
            
    except Exception as e:
        print(f"Error loading real data: {e}")
        return create_sample_data(15)


def process_real_data(df):
    """Process real UIDAI data into required format"""
    
    # Clean state names (from existing analysis.py)
    df["state_clean"] = df["state"].str.lower().str.strip()
    
    fix_map = {
        "orissa": "odisha",
        "pondicherry": "puducherry",
        "west bangal": "west bengal",
        "westbengal": "west bengal",
        "jammu & kashmir": "jammu and kashmir",
        "andaman & nicobar islands": "andaman and nicobar islands",
        "dadra & nagar haveli": "dadra and nagar haveli and daman and diu",
        "100000": None
    }
    
    df['state_clean'] = df['state_clean'].replace(fix_map)
    df = df[df['state_clean'].notna()]
    
    # Calculate total enrolments
    df["total_enrolment"] = (
        df["age_0_5"] + df["age_5_17"] + df["age_18_greater"]
    )
    
    # Group by district/state and calculate changes
    # This is simplified - in real implementation, you'd calculate actual percentage changes
    district_summary = (
        df.groupby("state_clean")["total_enrolment"]
        .sum()
        .reset_index()
        .rename(columns={'state_clean': 'district'})
    )
    
    # Add simulated percentage changes (in real system, calculate from historical data)
    np.random.seed(42)
    n_districts = len(district_summary)
    
    district_summary['enrolment_pct_change'] = np.random.normal(0, 15, n_districts)
    district_summary['update_pct_change'] = np.random.normal(-3, 12, n_districts)
    district_summary['child_pct_change'] = np.random.normal(1, 10, n_districts)
    district_summary['female_pct_change'] = np.random.normal(0, 8, n_districts)
    
    return district_summary


# ============================================================================
# EXPLORATORY DATA ANALYSIS (EDA) MODULE
# ============================================================================

class UIDaiEDA:
    """Exploratory Data Analysis for UIDAI Data"""

    def __init__(self):
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
            plt.style.use('default')
            sns.set_palette("husl")
            self.plt = plt
            self.sns = sns
            self.matplotlib_available = True
        except ImportError:
            print("Warning: matplotlib/seaborn not available. EDA plots will be skipped.")
            self.matplotlib_available = False

    def perform_complete_eda(self, df, save_plots=False, output_dir="eda_charts"):
        """Perform complete EDA analysis"""

        print("\n" + "=" * 80)
        print("EXPLORATORY DATA ANALYSIS (EDA)")
        print("=" * 80)

        if not self.matplotlib_available:
            print("Matplotlib not available - performing text-based analysis only")
            return self._text_based_eda(df)

        # Create output directory if saving plots
        if save_plots:
            import os
            os.makedirs(output_dir, exist_ok=True)

        # Basic statistics
        self._summary_statistics(df)

        # Distribution analysis
        self._distribution_analysis(df, save_plots, output_dir)

        # Correlation analysis
        self._correlation_analysis(df, save_plots, output_dir)

        # Anomaly analysis
        self._anomaly_analysis(df, save_plots, output_dir)

        # Geographic analysis (if available)
        self._geographic_analysis(df, save_plots, output_dir)

        print(f"\nEDA Complete! {'Plots saved to ' + output_dir if save_plots else ''}")

    def _summary_statistics(self, df):
        """Generate summary statistics"""

        print("\nðŸ“Š SUMMARY STATISTICS")
        print("-" * 50)

        numeric_cols = ['enrolment_pct_change', 'update_pct_change',
                       'child_pct_change', 'female_pct_change']

        print(f"Total Districts: {len(df)}")
        print(f"Date Range: {df['date'].min() if 'date' in df.columns else 'N/A'} to {df['date'].max() if 'date' in df.columns else 'N/A'}")

        print("\nNumeric Variables Summary:")
        print(df[numeric_cols].describe())

        print("\nMissing Values:")
        print(df[numeric_cols].isnull().sum())

    def _distribution_analysis(self, df, save_plots, output_dir):
        """Analyze distributions of key variables"""

        print("\nðŸ“ˆ DISTRIBUTION ANALYSIS")
        print("-" * 50)

        variables = ['enrolment_pct_change', 'update_pct_change',
                    'child_pct_change', 'female_pct_change']

        fig, axes = self.plt.subplots(2, 2, figsize=(15, 10))
        axes = axes.ravel()

        for i, var in enumerate(variables):
            # Histogram with KDE
            self.sns.histplot(data=df, x=var, kde=True, ax=axes[i])
            axes[i].set_title(f'Distribution of {var.replace("_", " ").title()}')
            axes[i].axvline(df[var].mean(), color='red', linestyle='--', label='.1f')
            axes[i].axvline(df[var].median(), color='green', linestyle='--', label='.1f')
            axes[i].legend()

        self.plt.tight_layout()

        if save_plots:
            self.plt.savefig(f"{output_dir}/distribution_analysis.png", dpi=300, bbox_inches='tight')
            print(f"Distribution plot saved to {output_dir}/distribution_analysis.png")

        self.plt.show()

    def _correlation_analysis(self, df, save_plots, output_dir):
        """Analyze correlations between variables"""

        print("\nðŸ”— CORRELATION ANALYSIS")
        print("-" * 50)

        numeric_cols = ['enrolment_pct_change', 'update_pct_change',
                       'child_pct_change', 'female_pct_change']

        corr_matrix = df[numeric_cols].corr()

        print("Correlation Matrix:")
        print(corr_matrix.round(3))

        # Heatmap
        self.plt.figure(figsize=(10, 8))
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
        self.sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
                        mask=mask, square=True, linewidths=0.5)
        self.plt.title('Correlation Heatmap of UIDAI Variables')

        if save_plots:
            self.plt.savefig(f"{output_dir}/correlation_heatmap.png", dpi=300, bbox_inches='tight')
            print(f"Correlation heatmap saved to {output_dir}/correlation_heatmap.png")

        self.plt.show()

    def _anomaly_analysis(self, df, save_plots, output_dir):
        """Analyze potential anomalies"""

        print("\nðŸš¨ ANOMALY ANALYSIS")
        print("-" * 50)

        # Detect anomalies using IQR method
        def detect_outliers_iqr(data):
            Q1 = data.quantile(0.25)
            Q3 = data.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            return (data < lower_bound) | (data > upper_bound)

        variables = ['enrolment_pct_change', 'update_pct_change',
                    'child_pct_change', 'female_pct_change']

        anomaly_counts = {}
        for var in variables:
            anomalies = detect_outliers_iqr(df[var])
            anomaly_counts[var] = anomalies.sum()
            print(f"{var}: {anomaly_counts[var]} potential anomalies")

        # Box plots to visualize anomalies
        fig, axes = self.plt.subplots(2, 2, figsize=(15, 10))
        axes = axes.ravel()

        for i, var in enumerate(variables):
            self.sns.boxplot(data=df, y=var, ax=axes[i])
            axes[i].set_title(f'Box Plot of {var.replace("_", " ").title()}')
            axes[i].axhline(df[var].median(), color='red', linestyle='--', alpha=0.7)

        self.plt.tight_layout()

        if save_plots:
            self.plt.savefig(f"{output_dir}/anomaly_boxplots.png", dpi=300, bbox_inches='tight')
            print(f"Anomaly boxplots saved to {output_dir}/anomaly_boxplots.png")

        self.plt.show()

    def _geographic_analysis(self, df, save_plots, output_dir):
        """Analyze geographic patterns if state/district data available"""

        print("\nðŸ—ºï¸ GEOGRAPHIC ANALYSIS")
        print("-" * 50)

        if 'state_clean' in df.columns:
            print("Top 10 States by District Count:")
            state_counts = df['state_clean'].value_counts().head(10)
            print(state_counts)

            # Bar chart of state distribution
            self.plt.figure(figsize=(12, 6))
            state_counts.plot(kind='bar')
            self.plt.title('District Distribution by State')
            self.plt.xlabel('State')
            self.plt.ylabel('Number of Districts')
            self.plt.xticks(rotation=45, ha='right')

            if save_plots:
                self.plt.savefig(f"{output_dir}/state_distribution.png", dpi=300, bbox_inches='tight')
                print(f"State distribution plot saved to {output_dir}/state_distribution.png")

            self.plt.show()

            # Average changes by state
            numeric_cols = ['enrolment_pct_change', 'update_pct_change',
                           'child_pct_change', 'female_pct_change']

            state_avg = df.groupby('state_clean')[numeric_cols].mean()

            print("\nAverage Changes by State (Top 10):")
            print(state_avg.head(10).round(2))

        else:
            print("No geographic data available for analysis")

    def generate_additional_charts(self, df, save_plots=False, output_dir="eda_charts"):
        """Generate additional visual charts for comprehensive analysis"""

        print("\nðŸ“Š ADDITIONAL VISUAL CHARTS")
        print("-" * 50)

        if not self.matplotlib_available:
            print("Matplotlib not available - skipping additional charts")
            return

        # Create output directory if saving plots
        if save_plots:
            import os
            os.makedirs(output_dir, exist_ok=True)

        # 1. Scatter plot matrix
        self._scatter_plot_matrix(df, save_plots, output_dir)

        # 2. Time series analysis (if date column exists)
        if 'date' in df.columns:
            self._time_series_analysis(df, save_plots, output_dir)

        # 3. Risk heat map
        self._risk_heatmap(df, save_plots, output_dir)

        # 4. Comparative bar charts
        self._comparative_barcharts(df, save_plots, output_dir)

        # 5. Pie charts for categorical data
        self._pie_charts(df, save_plots, output_dir)

        print(f"\nAdditional charts generated! {'Saved to ' + output_dir if save_plots else ''}")

    def _scatter_plot_matrix(self, df, save_plots, output_dir):
        """Generate scatter plot matrix for variable relationships"""

        numeric_cols = ['enrolment_pct_change', 'update_pct_change',
                       'child_pct_change', 'female_pct_change']

        # Pair plot
        pair_plot = self.sns.pairplot(df[numeric_cols], diag_kind='kde', plot_kws={'alpha': 0.6})
        pair_plot.fig.suptitle('Scatter Plot Matrix of UIDAI Variables', y=1.02)

        if save_plots:
            pair_plot.savefig(f"{output_dir}/scatter_matrix.png", dpi=300, bbox_inches='tight')
            print(f"Scatter plot matrix saved to {output_dir}/scatter_matrix.png")

        self.plt.show()

    def _time_series_analysis(self, df, save_plots, output_dir):
        """Time series analysis if date data is available"""

        print("Performing time series analysis...")

        # Convert date column if needed
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
            df_time = df.dropna(subset=['date']).sort_values('date')

            if len(df_time) > 0:
                # Time series line plots
                fig, axes = self.plt.subplots(2, 2, figsize=(15, 10))
                axes = axes.ravel()

                variables = ['enrolment_pct_change', 'update_pct_change',
                           'child_pct_change', 'female_pct_change']

                for i, var in enumerate(variables):
                    if var in df_time.columns:
                        df_time.groupby('date')[var].mean().plot(ax=axes[i])
                        axes[i].set_title(f'Time Series: {var.replace("_", " ").title()}')
                        axes[i].set_xlabel('Date')
                        axes[i].set_ylabel('Percentage Change')

                self.plt.tight_layout()

                if save_plots:
                    self.plt.savefig(f"{output_dir}/time_series_analysis.png", dpi=300, bbox_inches='tight')
                    print(f"Time series analysis saved to {output_dir}/time_series_analysis.png")

                self.plt.show()

    def _risk_heatmap(self, df, save_plots, output_dir):
        """Generate risk heatmap based on anomaly patterns"""

        # Create risk matrix
        risk_matrix = pd.DataFrame(index=df['district'].head(20))  # Top 20 districts

        # Calculate risk scores for each district
        for idx, row in df.head(20).iterrows():
            risk_score = 0
            if row.get('enrolment_pct_change', 0) < -30 or row.get('enrolment_pct_change', 0) > 30:
                risk_score += 1
            if row.get('update_pct_change', 0) < -25:
                risk_score += 1
            if row.get('child_pct_change', 0) < -20:
                risk_score += 1
            if row.get('female_pct_change', 0) < -15:
                risk_score += 1
            risk_matrix.loc[row['district'], 'Risk Score'] = risk_score

        if len(risk_matrix) > 0:
            # Heatmap
            self.plt.figure(figsize=(12, 8))
            self.sns.heatmap(risk_matrix.T, annot=True, cmap='Reds', cbar_kws={'label': 'Risk Level'})
            self.plt.title('District Risk Heatmap (Top 20 Districts)')
            self.plt.xlabel('District')
            self.plt.ylabel('Risk Metric')

            if save_plots:
                self.plt.savefig(f"{output_dir}/risk_heatmap.png", dpi=300, bbox_inches='tight')
                print(f"Risk heatmap saved to {output_dir}/risk_heatmap.png")

            self.plt.show()

    def _comparative_barcharts(self, df, save_plots, output_dir):
        """Generate comparative bar charts"""

        # Top 10 districts by enrolment change
        top_districts = df.nlargest(10, 'enrolment_pct_change')[['district', 'enrolment_pct_change']]

        self.plt.figure(figsize=(12, 6))
        self.sns.barplot(data=top_districts, x='district', y='enrolment_pct_change')
        self.plt.title('Top 10 Districts by Enrolment Change')
        self.plt.xlabel('District')
        self.plt.ylabel('Enrolment % Change')
        self.plt.xticks(rotation=45, ha='right')

        if save_plots:
            self.plt.savefig(f"{output_dir}/top_districts_enrolment.png", dpi=300, bbox_inches='tight')
            print(f"Top districts enrolment chart saved to {output_dir}/top_districts_enrolment.png")

        self.plt.show()

        # Bottom 10 districts by enrolment change
        bottom_districts = df.nsmallest(10, 'enrolment_pct_change')[['district', 'enrolment_pct_change']]

        self.plt.figure(figsize=(12, 6))
        self.sns.barplot(data=bottom_districts, x='district', y='enrolment_pct_change')
        self.plt.title('Bottom 10 Districts by Enrolment Change')
        self.plt.xlabel('District')
        self.plt.ylabel('Enrolment % Change')
        self.plt.xticks(rotation=45, ha='right')

        if save_plots:
            self.plt.savefig(f"{output_dir}/bottom_districts_enrolment.png", dpi=300, bbox_inches='tight')
            print(f"Bottom districts enrolment chart saved to {output_dir}/bottom_districts_enrolment.png")

        self.plt.show()

    def _pie_charts(self, df, save_plots, output_dir):
        """Generate pie charts for categorical analysis"""

        if 'state_clean' in df.columns:
            # State distribution pie chart
            state_counts = df['state_clean'].value_counts().head(8)  # Top 8 states

            self.plt.figure(figsize=(10, 8))
            self.plt.pie(state_counts.values, labels=state_counts.index, autopct='%1.1f%%',
                        startangle=90, colors=self.sns.color_palette("Set3"))
            self.plt.title('District Distribution by State (Top 8)')
            self.plt.axis('equal')

            if save_plots:
                self.plt.savefig(f"{output_dir}/state_pie_chart.png", dpi=300, bbox_inches='tight')
                print(f"State pie chart saved to {output_dir}/state_pie_chart.png")

            self.plt.show()

    def generate_comprehensive_report(self, df, save_plots=False, output_dir="eda_charts"):
        """Generate comprehensive visual report"""

        print("\nðŸ“ˆ COMPREHENSIVE VISUAL REPORT")
        print("=" * 50)

        # Run all analyses
        self.perform_complete_eda(df, save_plots, output_dir)
        self.generate_additional_charts(df, save_plots, output_dir)

        # Summary dashboard
        self._create_summary_dashboard(df, save_plots, output_dir)

        print("\nâœ… Comprehensive visual report completed!")

    def _create_summary_dashboard(self, df, save_plots, output_dir):
        """Create summary dashboard with key metrics"""

        fig, axes = self.plt.subplots(2, 3, figsize=(18, 10))

        # 1. Distribution of enrolment changes
        self.sns.histplot(data=df, x='enrolment_pct_change', kde=True, ax=axes[0,0])
        axes[0,0].set_title('Enrolment Change Distribution')
        axes[0,0].axvline(df['enrolment_pct_change'].mean(), color='red', linestyle='--')

        # 2. Box plot of all variables
        numeric_cols = ['enrolment_pct_change', 'update_pct_change', 'child_pct_change', 'female_pct_change']
        df_melted = df[numeric_cols].melt()
        self.sns.boxplot(data=df_melted, x='variable', y='value', ax=axes[0,1])
        axes[0,1].set_title('Variable Distributions (Box Plot)')
        axes[0,1].tick_params(axis='x', rotation=45)

        # 3. Correlation heatmap (simplified)
        corr = df[numeric_cols].corr()
        self.sns.heatmap(corr, annot=True, cmap='coolwarm', ax=axes[0,2], cbar=False)
        axes[0,2].set_title('Correlation Matrix')

        # 4. State distribution (if available)
        if 'state_clean' in df.columns:
            state_counts = df['state_clean'].value_counts().head(5)
            state_counts.plot(kind='bar', ax=axes[1,0])
            axes[1,0].set_title('Top 5 States by District Count')
            axes[1,0].tick_params(axis='x', rotation=45)
        else:
            axes[1,0].text(0.5, 0.5, 'No State Data Available', ha='center', va='center', transform=axes[1,0].transAxes)
            axes[1,0].set_title('State Distribution')

        # 5. Anomaly detection summary
        anomaly_vars = ['enrolment_pct_change', 'update_pct_change', 'child_pct_change', 'female_pct_change']
        anomaly_counts = []
        for var in anomaly_vars:
            Q1 = df[var].quantile(0.25)
            Q3 = df[var].quantile(0.75)
            IQR = Q3 - Q1
            outliers = ((df[var] < (Q1 - 1.5 * IQR)) | (df[var] > (Q3 + 1.5 * IQR))).sum()
            anomaly_counts.append(outliers)

        axes[1,1].bar(anomaly_vars, anomaly_counts)
        axes[1,1].set_title('Potential Anomalies by Variable')
        axes[1,1].tick_params(axis='x', rotation=45)

        # 6. Summary statistics table
        axes[1,2].axis('off')
        summary_text = ".0f"".0f"".0f"".0f"f"""
        SUMMARY STATISTICS
        Total Districts: {len(df)}
        Mean Enrolment Change: {df['enrolment_pct_change'].mean():.1f}%
        Median Enrolment Change: {df['enrolment_pct_change'].median():.1f}%
        Std Dev Enrolment: {df['enrolment_pct_change'].std():.1f}%
        Min Enrolment Change: {df['enrolment_pct_change'].min():.1f}%
        Max Enrolment Change: {df['enrolment_pct_change'].max():.1f}%
        """
        axes[1,2].text(0.1, 0.9, summary_text, transform=axes[1,2].transAxes, fontsize=10, verticalalignment='top')

        self.plt.tight_layout()

        if save_plots:
            self.plt.savefig(f"{output_dir}/summary_dashboard.png", dpi=300, bbox_inches='tight')
            print(f"Summary dashboard saved to {output_dir}/summary_dashboard.png")

        self.plt.show()

    def _text_based_eda(self, df):
        """Perform text-based EDA when matplotlib is not available"""

        print("Performing text-based EDA analysis...")

        self._summary_statistics(df)

        # Simple distribution analysis
        numeric_cols = ['enrolment_pct_change', 'update_pct_change',
                       'child_pct_change', 'female_pct_change']

        print("\nDISTRIBUTION ANALYSIS (Text-based)")
        print("-" * 40)

        for col in numeric_cols:
            print(f"\n{col.upper()}:")
            print(f"  Mean: {df[col].mean():.2f}")
            print(f"  Median: {df[col].median():.2f}")
            print(f"  Std Dev: {df[col].std():.2f}")
            print(f"  Min: {df[col].min():.2f}")
            print(f"  Max: {df[col].max():.2f}")
            print(f"  Skewness: {df[col].skew():.2f}")

        # Correlation
        print("\nCORRELATION ANALYSIS")
        print("-" * 40)
        corr_matrix = df[numeric_cols].corr()
        print(corr_matrix.round(3))

        return {"message": "Text-based EDA completed", "correlations": corr_matrix}


# ============================================================================
# EXAMPLE USAGE FUNCTIONS (From existing code)
# ============================================================================

def example_1_basic_classification():
    """Example 1: Basic classification workflow"""
    print("\n" + "=" * 80)
    print("EXAMPLE 1: Basic Risk Classification")
    print("=" * 80 + "\n")
    
    # Generate sample data
    df = create_sample_data(n_districts=5)
    
    # Run complete system
    system = UIDaiCompleteSystem()
    results = system.process_complete_pipeline(df)
    
    print("Classification Results:")
    print("-" * 80)
    print(results.to_string(index=False))
    
    return results


def example_2_high_priority_filtering():
    """Example 2: Filter and prioritize high-risk districts"""
    print("\n" + "=" * 80)
    print("EXAMPLE 2: High-Priority Alert System")
    print("=" * 80 + "\n")
    
    # Generate larger dataset
    df = create_sample_data(n_districts=15)
    
    # Run complete system
    system = UIDaiCompleteSystem()
    results = system.process_complete_pipeline(df)
    
    # Filter high-priority cases
    high_priority = results[results['Alert'] == 'Yes'].sort_values(
        'Risk Score', ascending=False
    )
    
    print(f"Total Districts Analyzed: {len(results)}")
    print(f"High-Priority Alerts: {len(high_priority)}")
    print("\nURGENT ACTION REQUIRED:\n")
    
    for idx, row in high_priority.iterrows():
        print(f"{idx+1}. {row['District']} - {row['Risk Type']}")
        print(f"   Risk Score: {row['Risk Score']:.2f}")
        print(f"   Action: {row['Action']}\n")
    
    return high_priority


def example_3_ml_validation():
    """Example 3: ML validation of classifications"""
    print("\n" + "=" * 80)
    print("EXAMPLE 3: ML Validation")
    print("=" * 80 + "\n")

    # Generate training data
    df = create_sample_data(n_districts=50)

    # Run pipeline to get rule-based results
    system = UIDaiCompleteSystem()

    # Get intermediate results for ML validation
    anomaly_data = system.model1.detect_anomalies(df)
    risk_data = system.model2.calculate_risk_score(anomaly_data)
    classification_results = system.model3.classify_batch(risk_data)

    print("Step 1: Rule-based classification complete")
    print(f"  Classified {len(classification_results)} districts\n")

    # ML validation
    print("Step 2: Training ML validator")
    ml_validator = Model3MLValidator(max_depth=3)
    ml_validator.train(risk_data, classification_results['Risk Type'])

    # Get ML predictions
    ml_predictions = ml_validator.predict(risk_data)

    # Validate
    validation = ml_validator.validate_against_rules(
        classification_results['Risk Type'],
        ml_predictions
    )

    print(f"Agreement Rate: {validation['agreement_rate']:.1%}")
    print(f"Validation Status: {validation['validation_status']}")

    return validation


def example_4_eda_analysis():
    """Example 4: Exploratory Data Analysis"""
    print("\n" + "=" * 80)
    print("EXAMPLE 4: EXPLORATORY DATA ANALYSIS (EDA)")
    print("=" * 80 + "\n")

    # Generate sample data
    df = create_sample_data(n_districts=100)

    # Add some state data for geographic analysis
    states = ['Maharashtra', 'Uttar Pradesh', 'Bihar', 'West Bengal', 'Madhya Pradesh',
              'Tamil Nadu', 'Rajasthan', 'Karnataka', 'Gujarat', 'Andhra Pradesh'] * 10
    df['state_clean'] = states[:len(df)]

    # Perform EDA
    eda = UIDaiEDA()
    eda.perform_complete_eda(df, save_plots=True, output_dir="eda_output")

    return {"message": "EDA completed", "plots_saved": True}


def main():
    """Run complete system demonstration"""
    print("\n" + "=" * 80)
    print("COMPLETE UIDAI SYSTEM - UNIFIED DEMONSTRATION")
    print("=" * 80)
    
    # Run all examples
    example_1_basic_classification()
    example_2_high_priority_filtering()
    example_3_ml_validation()
    example_4_eda_analysis()
    
    print("\n" + "=" * 80)
    print("ALL EXAMPLES COMPLETED SUCCESSFULLY")
    print("=" * 80)
    print("\nSystem is ready for production deployment")
    print("All models integrated and validated")


if __name__ == "__main__":
    main()